import signal
import sys
import time
import numpy as np
import threading
from config import RATE, CHUNK
from audio.recorder import AudioRecorder
from audio.stream_buffer import StreamBuffer
from stt.whisper_engine import transcribe_window
from llm.deepseek_client import DeepSeekClient
from ui.console_ui import ConsoleUI
from rich.console import Console
import webrtcvad

# åˆå§‹åŒ–æ§åˆ¶å°
console = Console()
# åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
recorder = AudioRecorder()  # éŸ³é¢‘å½•åˆ¶å™¨
buffer = StreamBuffer()  # éŸ³é¢‘ç¼“å†²åŒº
llm = DeepSeekClient()  # å¤§æ¨¡å‹å®¢æˆ·ç«¯
realtime_thread = None  # å®æ—¶å¤„ç†çº¿ç¨‹
stop_event = threading.Event()  # çº¿ç¨‹åœæ­¢äº‹ä»¶
realtime_vad = webrtcvad.Vad(3)  # å®æ—¶è¯­éŸ³æ´»åŠ¨æ£€æµ‹å™¨(é«˜çµæ•åº¦)

def start():
    """å¼€å§‹å½•éŸ³ä¸å®æ—¶å¤„ç†"""
    global realtime_thread, stop_event
    stop_event.clear()  
    ui.reset_realtime()  #
    recorder.start_recording()  
    buffer.clear()  
    console.print("ğŸ¤ å¼€å§‹å½•éŸ³ï¼Œå†æŒ‰ç©ºæ ¼ç»“æŸ")
    
    realtime_thread = threading.Thread(target=realtime_processing)
    realtime_thread.daemon = True 
    realtime_thread.start()

def is_voice_active(audio_chunk: np.ndarray, rate: int, chunk_size: int) -> bool:
    """
    åˆ¤æ–­éŸ³é¢‘ç‰‡æ®µæ˜¯å¦åŒ…å«æœ‰æ•ˆè¯­éŸ³
    param audio_chunk: éŸ³é¢‘æ•°æ®
    param rate: é‡‡æ ·ç‡
    param chunk_size: å—å¤§å°
    return: åŒ…å«æœ‰æ•ˆè¯­éŸ³è¿”å›Trueï¼Œå¦åˆ™False
    """
    audio_bytes = audio_chunk.tobytes()
    frame_duration = chunk_size * 1000 // rate
    if frame_duration not in [10, 20, 30]:
        return False
    
    frame_count = 0  # æ€»å¸§æ•°
    voice_frame_count = 0  # è¯­éŸ³å¸§æ•°
    for i in range(0, len(audio_bytes), chunk_size * 2): 
        frame = audio_bytes[i:i + chunk_size * 2]
        if len(frame) < chunk_size * 2:
            break
        frame_count += 1
        if realtime_vad.is_speech(frame, rate):
            voice_frame_count += 1
    
    return voice_frame_count / frame_count > 0.5 if frame_count > 0 else False

def realtime_processing():
    """å®æ—¶éŸ³é¢‘å¤„ç†çº¿ç¨‹ï¼šè¯­éŸ³æ£€æµ‹ä¸å®æ—¶è½¬æ–‡å­—"""
    last_voice_time = time.time()  # æœ€åä¸€æ¬¡æ£€æµ‹åˆ°è¯­éŸ³çš„æ—¶é—´
    silence_timeout = 2  # é™éŸ³è¶…æ—¶æ—¶é—´
    last_text = ""  # ä¸Šä¸€æ¬¡è¯†åˆ«çš„æ–‡æœ¬
    while not stop_event.is_set() and recorder.is_recording:
        audio_chunk = np.array(recorder.full_audio[-RATE*5:], dtype="int16")
        if len(audio_chunk) < RATE*0.5:
            time.sleep(0.2)
            continue
        
        if is_voice_active(audio_chunk, RATE, CHUNK):
            last_voice_time = time.time()  
            current_text = transcribe_window(audio_chunk)
            if current_text and current_text != last_text:
                ui.update_realtime(current_text)
                console.print(f"[å®æ—¶] {current_text}")
                last_text = current_text  
        else:
            if time.time() - last_voice_time > silence_timeout:
                pass
            else:
                time.sleep(0.2)
                continue
        
        time.sleep(0.3) 

def stop():
    """åœæ­¢å½•éŸ³ä¸å®æ—¶å¤„ç†ï¼Œæ‰§è¡Œåç»­æµç¨‹"""
    global stop_event
    stop_event.set()  
    recorder.stop_recording() 
    if realtime_thread:
        realtime_thread.join()
    
    console.print("â¹ï¸ å½•éŸ³ç»“æŸï¼Œæ•´åˆæ–‡æœ¬â€¦")
    final_text = ui.get_last_realtime_text().strip()
    if not final_text:
        console.print("æœªæ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³å†…å®¹")
        return
    
    ConsoleUI.print_asr(final_text)
    edited_text = ConsoleUI.ask_edit(final_text)
    console.print(f"[ASK] {edited_text}")
    
    for token in llm.chat(edited_text):
        ConsoleUI.print_assistant(token)
    console.print()

ui = ConsoleUI(start, stop)

def sigint_handler(*_):
    """å¤„ç†Ctrl+Cé€€å‡ºä¿¡å·"""
    console.print("\né€€å‡º")
    sys.exit(0)

signal.signal(signal.SIGINT, sigint_handler)

if __name__ == "__main__":
    recorder.start()  
    console.print("ç¨‹åºå·²å¯åŠ¨ï¼ŒæŒ‰ç©ºæ ¼å¼€å§‹/åœæ­¢å½•éŸ³ï¼ŒCtrl+C é€€å‡º", style="bold green")
    while True:
        time.sleep(0.1)